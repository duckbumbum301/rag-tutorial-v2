#!/usr/bin/env python3
"""
Model Response Analyzer: So s√°nh ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi c·ªßa c√°c models
"""

import re
import time
from multi_model_evaluation import query_rag_with_model, MODELS

def analyze_medical_response(response: str, query: str) -> dict:
    """Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi y t·∫ø"""
    
    analysis = {
        'language_quality': 0,
        'medical_accuracy': 0,
        'completeness': 0,
        'structure': 0,
        'safety': 0,
        'total_score': 0,
        'details': {}
    }
    
    # 1. Language Quality (Vietnamese vs English) - 25 points
    vietnamese_words = len(re.findall(r'[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]', response.lower()))
    english_words = len(re.findall(r'[a-zA-Z]+', response))
    
    if vietnamese_words > english_words * 0.5:
        analysis['language_quality'] = 25
        analysis['details']['language'] = "‚úÖ Vietnamese dominant (good)"
    elif vietnamese_words > 0:
        analysis['language_quality'] = 15
        analysis['details']['language'] = "‚ö†Ô∏è Mixed Vietnamese/English"
    else:
        analysis['language_quality'] = 5
        analysis['details']['language'] = "‚ùå English only"
    
    # 2. Medical Accuracy - 30 points
    medical_terms = {
        'ho g√†': ['ho g√†', 'whooping cough', 'pertussis', 'bordetella', 'c∆°n ho', 'ti√™m ch·ªßng'],
        'vi√™m ph·ªïi': ['vi√™m ph·ªïi', 'pneumonia', 'kh√≥ th·ªü', 's·ªët', 'ho', 'ƒëau ng·ª±c'],
        's·ªët': ['s·ªët', 'fever', 'nhi·ªát ƒë·ªô', '38¬∞c', '39¬∞c', 'h·∫° s·ªët'],
        'ti√™u ch·∫£y': ['ti√™u ch·∫£y', 'diarrhea', 'ph√¢n l·ªèng', 'm·∫•t n∆∞·ªõc', 'ors']
    }
    
    query_lower = query.lower()
    response_lower = response.lower()
    
    # Identify medical condition from query
    condition = None
    for cond, terms in medical_terms.items():
        if any(term in query_lower for term in terms[:2]):
            condition = cond
            break
    
    if condition:
        relevant_terms = medical_terms[condition]
        found_terms = sum(1 for term in relevant_terms if term in response_lower)
        analysis['medical_accuracy'] = min(30, (found_terms / len(relevant_terms)) * 30)
        analysis['details']['medical_terms'] = f"Found {found_terms}/{len(relevant_terms)} relevant terms"
    else:
        analysis['medical_accuracy'] = 15
        analysis['details']['medical_terms'] = "General medical content"
    
    # 3. Completeness - 20 points
    response_length = len(response)
    if response_length > 200:
        analysis['completeness'] = 20
        analysis['details']['length'] = "‚úÖ Comprehensive answer"
    elif response_length > 100:
        analysis['completeness'] = 15
        analysis['details']['length'] = "‚ö†Ô∏è Adequate length"
    else:
        analysis['completeness'] = 10
        analysis['details']['length'] = "‚ùå Too brief"
    
    # 4. Structure - 15 points
    structure_points = 0
    if '‚Ä¢' in response or '-' in response or '1.' in response:
        structure_points += 5
        analysis['details']['bullets'] = "‚úÖ Has bullet points"
    
    if any(header in response for header in ['##', '**', 'Tri·ªáu ch·ª©ng', 'Nguy√™n nh√¢n']):
        structure_points += 5
        analysis['details']['headers'] = "‚úÖ Has headers/formatting"
    
    if len(response.split('\n')) > 3:
        structure_points += 5
        analysis['details']['paragraphs'] = "‚úÖ Multi-paragraph"
    
    analysis['structure'] = structure_points
    
    # 5. Safety - 10 points
    safety_indicators = ['tham kh·∫£o b√°c sƒ©', 'c·∫ßn kh√°m', 'nghi√™m tr·ªçng', 'c·∫£nh b√°o', 'an to√†n']
    safety_count = sum(1 for indicator in safety_indicators if indicator in response_lower)
    
    if safety_count >= 2:
        analysis['safety'] = 10
        analysis['details']['safety'] = "‚úÖ Good safety warnings"
    elif safety_count >= 1:
        analysis['safety'] = 7
        analysis['details']['safety'] = "‚ö†Ô∏è Some safety mentions"
    else:
        analysis['safety'] = 3
        analysis['details']['safety'] = "‚ùå No safety warnings"
    
    # Calculate total score
    analysis['total_score'] = (
        analysis['language_quality'] + 
        analysis['medical_accuracy'] + 
        analysis['completeness'] + 
        analysis['structure'] + 
        analysis['safety']
    )
    
    return analysis

def compare_models_detailed(query: str):
    """So s√°nh chi ti·∫øt 3 models cho 1 query"""
    
    print(f"üîç DETAILED ANALYSIS FOR: '{query}'")
    print("=" * 80)
    
    results = {}
    
    # Get responses from all models
    for model_name, config in MODELS.items():
        print(f"\nü§ñ Testing {config['name']}...")
        
        start_time = time.time()
        response = query_rag_with_model(query, model_name, show_sources=False)
        end_time = time.time()
        
        # Analyze response quality
        analysis = analyze_medical_response(response, query)
        
        results[model_name] = {
            'name': config['name'],
            'response': response,
            'time': end_time - start_time,
            'analysis': analysis
        }
        
        print(f"   ‚è±Ô∏è Time: {end_time - start_time:.1f}s")
        print(f"   üìä Score: {analysis['total_score']}/100")
    
    # Generate comparison report
    print("\nüìä DETAILED COMPARISON REPORT")
    print("=" * 80)
    
    # Sort by total score
    sorted_results = sorted(results.items(), key=lambda x: x[1]['analysis']['total_score'], reverse=True)
    
    for rank, (model_name, result) in enumerate(sorted_results, 1):
        analysis = result['analysis']
        
        print(f"\nüèÜ RANK {rank}: {result['name']}")
        print(f"   Overall Score: {analysis['total_score']}/100 ({analysis['total_score']}%)")
        print(f"   Response Time: {result['time']:.1f}s")
        print()
        
        # Detailed breakdown
        print("   üìä Score Breakdown:")
        print(f"      Language Quality: {analysis['language_quality']}/25 - {analysis['details'].get('language', '')}")
        print(f"      Medical Accuracy: {analysis['medical_accuracy']:.1f}/30 - {analysis['details'].get('medical_terms', '')}")
        print(f"      Completeness: {analysis['completeness']}/20 - {analysis['details'].get('length', '')}")
        print(f"      Structure: {analysis['structure']}/15 - {analysis['details'].get('bullets', '')} {analysis['details'].get('headers', '')}")
        print(f"      Safety: {analysis['safety']}/10 - {analysis['details'].get('safety', '')}")
        
        # Show response preview
        print(f"\n   üìù Response Preview:")
        preview = result['response'][:200] + "..." if len(result['response']) > 200 else result['response']
        print(f"      {preview}")
        
        print("-" * 60)
    
    # Winner announcement
    winner = sorted_results[0]
    print(f"\nü•á WINNER: {winner[1]['name']}")
    print(f"   Score: {winner[1]['analysis']['total_score']}/100")
    print(f"   Best for: Vietnamese medical queries")
    
    return sorted_results

def quick_comparison(query: str):
    """So s√°nh nhanh ch·ªâ v·ªÅ accuracy v√† speed"""
    
    print(f"‚ö° QUICK COMPARISON: '{query}'")
    print("-" * 50)
    
    results = {}
    
    for model_name, config in MODELS.items():
        start_time = time.time()
        response = query_rag_with_model(query, model_name)
        end_time = time.time()
        
        # Quick analysis
        vietnamese_ratio = len(re.findall(r'[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]', response.lower())) / max(len(response), 1)
        length = len(response)
        time_taken = end_time - start_time
        
        results[model_name] = {
            'name': config['name'],
            'vietnamese_ratio': vietnamese_ratio,
            'length': length,
            'time': time_taken,
            'response': response
        }
    
    # Quick ranking
    for model_name, result in results.items():
        print(f"{result['name']:<25} | "
              f"VI: {result['vietnamese_ratio']*100:.0f}% | "
              f"Length: {result['length']:<4} | "
              f"Time: {result['time']:.1f}s")
    
    # Find best
    best_vn = max(results.items(), key=lambda x: x[1]['vietnamese_ratio'])
    fastest = min(results.items(), key=lambda x: x[1]['time'])
    
    print(f"\nüèÜ Best Vietnamese: {best_vn[1]['name']} ({best_vn[1]['vietnamese_ratio']*100:.0f}%)")
    print(f"‚ö° Fastest: {fastest[1]['name']} ({fastest[1]['time']:.1f}s)")

def main():
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python compare_models.py <query> [--detailed]")
        print("Example: python compare_models.py 'Ho g√† ·ªü tr·∫ª em' --detailed")
        return
    
    query = sys.argv[1]
    detailed = "--detailed" in sys.argv
    
    print("üöÄ MODEL RESPONSE COMPARISON TOOL")
    print("=" * 60)
    
    if detailed:
        compare_models_detailed(query)
    else:
        quick_comparison(query)

if __name__ == "__main__":
    main()