import argparse
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM

from get_embedding_function import get_embedding_function

CHROMA_PATH = "chroma"

# Query type definitions and configurations
QUERY_TYPES = {
    "symptom": {
        "keywords": ["tri·ªáu ch·ª©ng", "bi·ªÉu hi·ªán", "d·∫•u hi·ªáu", "symptoms", "signs", "c√≥ th·ªÉ", "th∆∞·ªùng"],
        "k": 3,  # Need fewer chunks for symptom lists
        "template": """
D·ª±a tr√™n th√¥ng tin y t·∫ø sau, h√£y li·ªát k√™ c√°c tri·ªáu ch·ª©ng ch√≠nh:

{context}

---

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. Li·ªát k√™ tri·ªáu ch·ª©ng ch√≠nh tr∆∞·ªõc
2. Sau ƒë√≥ n√™u nguy√™n nh√¢n (n·∫øu c√≥)
3. Cu·ªëi c√πng l√† c√°c d·∫•u hi·ªáu c·∫£nh b√°o nguy hi·ªÉm
4. S·ª≠ d·ª•ng format danh s√°ch r√µ r√†ng
"""
    },
    "dosage": {
        "keywords": ["li·ªÅu", "dose", "mg", "kg", "ml", "l·∫ßn", "ng√†y", "tu·ªïi", "c√¢n n·∫∑ng"],
        "k": 4,  # Need more context for dosage calculations
        "template": """
D·ª±a tr√™n th√¥ng tin d∆∞·ª£c l√Ω sau, t√≠nh to√°n li·ªÅu d√πng thu·ªëc:

{context}

---

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n t√≠nh li·ªÅu:
1. X√°c ƒë·ªãnh li·ªÅu/kg c√¢n n·∫∑ng
2. C√¥ng th·ª©c: (li·ªÅu/kg) √ó c√¢n n·∫∑ng b·ªánh nh√¢n
3. T·∫ßn su·∫•t s·ª≠ d·ª•ng (s·ªë l·∫ßn/ng√†y)
4. Th·ªùi gian ƒëi·ªÅu tr·ªã
5. L∆ØU √ù an to√†n v√† ch·ªëng ch·ªâ ƒë·ªãnh

Format b·∫£ng: C√¢n n·∫∑ng | Li·ªÅu d√πng | T·∫ßn su·∫•t
"""
    },
    "procedure": {
        "keywords": ["c√°ch", "quy tr√¨nh", "th·ªß thu·∫≠t", "l√†m th·∫ø n√†o", "how to", "steps", "b∆∞·ªõc"],
        "k": 5,  # Need detailed step-by-step context
        "template": """
D·ª±a tr√™n h∆∞·ªõng d·∫´n y t·∫ø sau, m√¥ t·∫£ quy tr√¨nh th·ª±c hi·ªán:

{context}

---

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n tr√¨nh b√†y:
1. Chia th√†nh c√°c b∆∞·ªõc c√≥ ƒë√°nh s·ªë r√µ r√†ng
2. S·ª≠ d·ª•ng bullet points cho c√°c ƒëi·ªÉm quan tr·ªçng
3. N√™u r√µ d·ª•ng c·ª• c·∫ßn thi·∫øt
4. L∆∞u √Ω an to√†n v√† bi·∫øn ch·ª©ng
5. Khi n√†o c·∫ßn tham kh·∫£o b√°c sƒ©
"""
    },
    "definition": {
        "keywords": ["l√† g√¨", "ƒë·ªãnh nghƒ©a", "what is", "kh√°i ni·ªám", "c√≥ nghƒ©a l√†"],
        "k": 2,  # Simple definition needs fewer chunks
        "template": """
D·ª±a tr√™n ki·∫øn th·ª©c y khoa sau, ƒë·ªãnh nghƒ©a thu·∫≠t ng·ªØ:

{context}

---

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. ƒê·ªãnh nghƒ©a ch√≠nh x√°c v√† ng·∫Øn g·ªçn
2. Gi·∫£i th√≠ch b·∫±ng ng√¥n ng·ªØ d·ªÖ hi·ªÉu
3. N√™u c√°c ƒë·∫∑c ƒëi·ªÉm quan tr·ªçng
4. ƒê∆∞a ra v√≠ d·ª• minh h·ªça n·∫øu c·∫ßn
"""
    },
    "contraindication": {
        "keywords": ["an to√†n", "ch·ªëng ch·ªâ ƒë·ªãnh", "t√°c d·ª•ng ph·ª•", "safe", "side effects", "nguy hi·ªÉm"],
        "k": 6,  # Need comprehensive safety information
        "template": """
D·ª±a tr√™n th√¥ng tin an to√†n y t·∫ø sau, ƒë√°nh gi√° ƒë·ªô an to√†n:

{context}

---

C√¢u h·ªèi: {question}

‚ö†Ô∏è H∆Ø·ªöNG D·∫™N AN TO√ÄN:
1. LU√îN n√™u c√°c ch·ªëng ch·ªâ ƒë·ªãnh tr∆∞·ªõc
2. Li·ªát k√™ t√°c d·ª•ng ph·ª• c√≥ th·ªÉ x·∫£y ra
3. Nh√≥m ƒë·ªëi t∆∞·ª£ng c·∫ßn th·∫≠n tr·ªçng
4. Li·ªÅu l∆∞·ª£ng an to√†n
5. D·∫•u hi·ªáu c·∫ßn ng·ª´ng s·ª≠ d·ª•ng ngay

üö® KH√îNG BAO GI·ªú b·ªè qua th√¥ng tin v·ªÅ ch·ªëng ch·ªâ ƒë·ªãnh!
"""
    },
    "interaction": {
        "keywords": ["t∆∞∆°ng t√°c", "k·∫øt h·ª£p", "d√πng chung", "interaction", "together"],
        "k": 4,
        "template": """
D·ª±a tr√™n d·ªØ li·ªáu t∆∞∆°ng t√°c thu·ªëc sau, ph√¢n t√≠ch kh·∫£ nƒÉng t∆∞∆°ng t√°c:

{context}

---

C√¢u h·ªèi: {question}

Ph√¢n t√≠ch t∆∞∆°ng t√°c:
1. Lo·∫°i t∆∞∆°ng t√°c (tƒÉng/gi·∫£m hi·ªáu qu·∫£, ƒë·ªôc t√≠nh)
2. M·ª©c ƒë·ªô nghi√™m tr·ªçng (nh·∫π/trung b√¨nh/n·∫∑ng)
3. C∆° ch·∫ø t∆∞∆°ng t√°c
4. Khuy·∫øn ngh·ªã ƒëi·ªÅu ch·ªânh li·ªÅu
5. Theo d√µi c√°c d·∫•u hi·ªáu b·∫•t th∆∞·ªùng
"""
    }
}

# Default template for unclassified queries
DEFAULT_PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""


def main():
    # Create CLI.
    parser = argparse.ArgumentParser()
    parser.add_argument("query_text", type=str, help="The query text.")
    parser.add_argument("--show-sources", action="store_true", help="Show source documents.")
    args = parser.parse_args()
    query_text = args.query_text
    query_rag(query_text, show_sources=args.show_sources)


def detect_query_type(query: str) -> str:
    """
    Detect query type using keyword matching and pattern recognition.
    """
    query_lower = query.lower()
    
    # Score each query type based on keyword matches
    type_scores = {}
    
    for query_type, config in QUERY_TYPES.items():
        score = 0
        keywords = config["keywords"]
        
        for keyword in keywords:
            if keyword in query_lower:
                score += 1
        
        # Bonus scoring for specific patterns
        if query_type == "dosage":
            # Look for weight/age patterns
            import re
            if re.search(r'\d+\s*(kg|tu·ªïi|nƒÉm|th√°ng)', query_lower):
                score += 2
            if re.search(r'(mg|ml|li·ªÅu)', query_lower):
                score += 2
        
        elif query_type == "procedure":
            # Look for question patterns about "how"
            if any(pattern in query_lower for pattern in ["l√†m th·∫ø n√†o", "c√°ch n√†o", "quy tr√¨nh"]):
                score += 2
        
        elif query_type == "symptom":
            # Look for symptom inquiry patterns
            if any(pattern in query_lower for pattern in ["c√≥ tri·ªáu ch·ª©ng g√¨", "bi·ªÉu hi·ªán nh∆∞ th·∫ø n√†o"]):
                score += 2
        
        type_scores[query_type] = score
    
    # Return type with highest score, or 'general' if no clear match
    if max(type_scores.values()) > 0:
        return max(type_scores.items(), key=lambda x: x[1])[0]
    else:
        return "general"


def get_prompt_template(query_type: str, query: str, context: str) -> str:
    """
    Get specialized prompt template based on query type.
    """
    if query_type in QUERY_TYPES:
        return QUERY_TYPES[query_type]["template"].format(context=context, question=query)
    else:
        return DEFAULT_PROMPT_TEMPLATE.format(context=context, question=query)


def get_optimal_k(query_type: str) -> int:
    """
    Get optimal number of chunks to retrieve based on query type.
    """
    if query_type in QUERY_TYPES:
        return QUERY_TYPES[query_type]["k"]
    else:
        return 5  # Default k


def format_response_by_type(response: str, query_type: str) -> str:
    """
    Apply type-specific formatting to response.
    """
    if query_type == "symptom":
        # Ensure markdown list formatting
        lines = response.split('\n')
        formatted_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith('-') and not line.startswith('*') and not line.startswith('1.'):
                # Check if it looks like a symptom item
                if any(indicator in line.lower() for indicator in ['tri·ªáu ch·ª©ng', 'bi·ªÉu hi·ªán', 'c√≥ th·ªÉ', 'th∆∞·ªùng']):
                    if ',' in line or ';' in line:
                        # Split compound symptoms
                        symptoms = [s.strip() for s in line.replace(';', ',').split(',') if s.strip()]
                        for symptom in symptoms:
                            formatted_lines.append(f"‚Ä¢ {symptom}")
                    else:
                        formatted_lines.append(f"‚Ä¢ {line}")
                else:
                    formatted_lines.append(line)
            else:
                formatted_lines.append(line)
        return '\n'.join(formatted_lines)
    
    elif query_type == "dosage":
        # Try to structure dosage information in table format
        import re
        lines = response.split('\n')
        formatted_lines = ["## üíä TH√îNG TIN LI·ªÄU D√ôNG\n"]
        
        dosage_found = False
        for line in lines:
            # Look for dosage patterns
            if re.search(r'\d+.*mg|\d+.*ml|\d+.*kg', line):
                dosage_found = True
                formatted_lines.append(f"üìã **{line.strip()}**")
            elif 'li·ªÅu' in line.lower() or 'dose' in line.lower():
                formatted_lines.append(f"üî¢ {line.strip()}")
            elif line.strip():
                formatted_lines.append(line.strip())
        
        if dosage_found:
            formatted_lines.append("\n‚ö†Ô∏è **L∆∞u √Ω**: Lu√¥n tham kh·∫£o √Ω ki·∫øn b√°c sƒ© tr∆∞·ªõc khi s·ª≠ d·ª•ng thu·ªëc.")
        
        return '\n'.join(formatted_lines)
    
    elif query_type == "procedure":
        # Format as numbered steps
        lines = response.split('\n')
        formatted_lines = ["## üìã QUY TR√åNH TH·ª∞C HI·ªÜN\n"]
        
        step_counter = 1
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                # Check if it's a step
                if any(indicator in line.lower() for indicator in ['b∆∞·ªõc', 'step', 'ƒë·∫ßu ti√™n', 'sau ƒë√≥', 'cu·ªëi c√πng']):
                    formatted_lines.append(f"{step_counter}. **{line}**")
                    step_counter += 1
                elif line.startswith('‚Ä¢') or line.startswith('-'):
                    formatted_lines.append(f"   {line}")
                else:
                    formatted_lines.append(line)
        
        formatted_lines.append("\n‚ö†Ô∏è **An to√†n**: Th·ª±c hi·ªán theo ƒë√∫ng h∆∞·ªõng d·∫´n v√† tham kh·∫£o chuy√™n gia khi c·∫ßn thi·∫øt.")
        return '\n'.join(formatted_lines)
    
    elif query_type == "contraindication":
        # Add warning formatting
        warning_response = f"""üö® **TH√îNG TIN AN TO√ÄN QUAN TR·ªåNG**

{response}

‚ö†Ô∏è **C·∫¢NH B√ÅO**: Th√¥ng tin n√†y ch·ªâ mang t√≠nh ch·∫•t tham kh·∫£o. LU√îN tham kh·∫£o √Ω ki·∫øn b√°c sƒ© tr∆∞·ªõc khi s·ª≠ d·ª•ng b·∫•t k·ª≥ lo·∫°i thu·ªëc n√†o."""
        return warning_response
    
    else:
        # Default formatting
        return response


def rerank_results(results, query_text: str, weights=None) -> list:
    """
    Semantic re-ranking layer to improve relevance filtering beyond cosine similarity.
    
    Args:
        results: List of (Document, score) tuples from initial retrieval
        query_text: Original query text
        weights: Custom weights dict for different factors
    
    Returns:
        List of (Document, enhanced_score, confidence, reasoning) tuples
    """
    import re
    from typing import Dict, List, Tuple, Any
    
    if weights is None:
        weights = {
            'medical_keywords': 1.5,
            'medical_units': 1.3,
            'question_type': 1.2,
            'coherence': 1.4,
            'diversity': 1.1,
            'chunk_quality': 1.3
        }
    
    def get_medical_keyword_boost(text: str, query: str) -> float:
        """Calculate boost based on medical keyword matching"""
        medical_keywords = {
            'tri·ªáu ch·ª©ng': ['tri·ªáu ch·ª©ng', 'd·∫•u hi·ªáu', 'bi·ªÉu hi·ªán', 'c√≥ th·ªÉ'],
            'ch·∫©n ƒëo√°n': ['ch·∫©n ƒëo√°n', 'x√©t nghi·ªám', 'kh√°m', 'ph√°t hi·ªán', 'nh·∫≠n bi·∫øt'],
            'ƒëi·ªÅu tr·ªã': ['ƒëi·ªÅu tr·ªã', 'ch·ªØa tr·ªã', 'thu·ªëc', 'ph∆∞∆°ng ph√°p', 'c√°ch ch·ªØa'],
            'li·ªÅu d√πng': ['li·ªÅu', 'mg', 'ml', 'l·∫ßn', 'ng√†y', 'd√πng'],
            'ph√≤ng ng·ª´a': ['ph√≤ng ng·ª´a', 'd·ª± ph√≤ng', 'tr√°nh', 'ngƒÉn ch·∫∑n'],
            'nguy√™n nh√¢n': ['nguy√™n nh√¢n', 'do', 'g√¢y ra', 'v√¨ sao', 't·∫°i sao']
        }
        
        text_lower = text.lower()
        query_lower = query.lower()
        
        boost = 1.0
        
        # Identify query intent
        query_intent = None
        for intent, keywords in medical_keywords.items():
            if any(kw in query_lower for kw in keywords):
                query_intent = intent
                break
        
        if query_intent:
            # Check if content matches query intent
            intent_keywords = medical_keywords[query_intent]
            matches = sum(1 for kw in intent_keywords if kw in text_lower)
            if matches > 0:
                boost *= weights['medical_keywords']
        
        return boost
    
    def get_medical_units_boost(text: str) -> float:
        """Calculate boost for medical units and measurements"""
        unit_patterns = [
            r'\d+\s*(mg|ml|kg|g|¬∞C|tu·ªïi|th√°ng|nƒÉm)',
            r'\d+\s*(l·∫ßn|ng√†y|gi·ªù|ph√∫t)',
            r'\d+-\d+\s*(mg|ml|kg)',
            r'li·ªÅu\s*l∆∞·ª£ng',
            r'tr·ªçng\s*l∆∞·ª£ng'
        ]
        
        text_lower = text.lower()
        unit_matches = sum(1 for pattern in unit_patterns if re.search(pattern, text_lower))
        
        if unit_matches > 0:
            return weights['medical_units']
        return 1.0
    
    def get_question_type_boost(text: str, query: str) -> float:
        """Calculate boost based on question type matching"""
        question_patterns = {
            'what': ['l√† g√¨', 'g√¨ l√†', 'ƒë·ªãnh nghƒ©a', 'kh√°i ni·ªám'],
            'how': ['nh∆∞ th·∫ø n√†o', 'c√°ch n√†o', 'l√†m sao', 'quy tr√¨nh', 'th·ªß thu·∫≠t'],
            'when': ['khi n√†o', 'l√∫c n√†o', 'th·ªùi ƒëi·ªÉm', 'th·ªùi gian'],
            'why': ['t·∫°i sao', 'v√¨ sao', 'nguy√™n nh√¢n', 'do ƒë√¢u'],
            'where': ['·ªü ƒë√¢u', 'v·ªã tr√≠', 'ch·ªó n√†o'],
            'how_much': ['bao nhi√™u', 'm·ª©c ƒë·ªô', 's·ªë l∆∞·ª£ng', 'li·ªÅu']
        }
        
        query_lower = query.lower()
        text_lower = text.lower()
        
        # Identify question type
        question_type = None
        for qtype, patterns in question_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                question_type = qtype
                break
        
        if question_type and question_type in question_patterns:
            # Check if answer pattern matches question type
            answer_indicators = {
                'what': ['l√†', 'ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a', 'c√≥ nghƒ©a'],
                'how': ['b∆∞·ªõc', 'giai ƒëo·∫°n', 'c√°ch', 'ph∆∞∆°ng ph√°p'],
                'when': ['khi', 'l√∫c', 'sau khi', 'tr∆∞·ªõc khi'],
                'why': ['do', 'v√¨', 'nguy√™n nh√¢n', 'g√¢y ra'],
                'where': ['t·∫°i', '·ªü', 'v√πng', 'khu v·ª±c'],
                'how_much': ['mg', 'ml', 'kg', 'l·∫ßn', 'li·ªÅu']
            }
            
            if question_type in answer_indicators:
                indicators = answer_indicators[question_type]
                if any(ind in text_lower for ind in indicators):
                    return weights['question_type']
        
        return 1.0
    
    def get_coherence_score(chunk_text: str, query: str) -> float:
        """Calculate coherence score for logical sequence"""
        # Check for structured content
        structure_indicators = [
            r'^\d+\.\s+',  # Numbered lists
            r'[Bb]∆∞·ªõc\s+\d+',  # Steps
            r'[Gg]iai\s+ƒëo·∫°n\s+\d+',  # Phases
            r'[Tt]ri·ªáu\s+ch·ª©ng.*:',  # Symptom lists
            r'[Nn]guy√™n\s+nh√¢n.*:'  # Cause lists
        ]
        
        coherence = 1.0
        
        # Bonus for structured content
        for pattern in structure_indicators:
            if re.search(pattern, chunk_text, re.MULTILINE):
                coherence *= 1.2
                break
        
        # Check for completeness
        if len(chunk_text.strip()) > 100:  # Substantial content
            coherence *= 1.1
        
        # Penalty for fragmented content
        if chunk_text.count('...') > 2:
            coherence *= 0.8
        
        return min(coherence * weights['coherence'], 2.0)
    
    def get_diversity_score(results_list: List, current_doc) -> float:
        """Calculate diversity score to avoid redundant sources"""
        current_source = current_doc.metadata.get('source', '')
        current_page = current_doc.metadata.get('page', -1)
        
        # Check for same source/page redundancy
        same_page_count = sum(1 for doc, _ in results_list 
                             if doc.metadata.get('source', '') == current_source and 
                                doc.metadata.get('page', -1) == current_page)
        
        if same_page_count > 1:
            return 0.7  # Penalty for redundancy
        
        return weights['diversity']
    
    def get_chunk_quality_score(doc) -> float:
        """Score based on chunk metadata quality"""
        metadata = doc.metadata
        
        score = 1.0
        
        # Bonus for high relevance score from chunking
        if 'relevance_score' in metadata:
            relevance = metadata['relevance_score']
            score *= (1 + relevance * 0.5)  # Up to 1.5x boost
        
        # Bonus for important content types
        content_type = metadata.get('content_type', '')
        if '[PROCEDURE]' in content_type or '[TABLE]' in content_type:
            score *= 1.2
        
        return min(score * weights['chunk_quality'], 2.0)
    
    def generate_reasoning(doc, boosts: Dict) -> str:
        """Generate human-readable reasoning for ranking"""
        reasons = []
        
        if boosts.get('medical_keywords', 1.0) > 1.0:
            reasons.append("strong medical keyword match")
        
        if boosts.get('medical_units', 1.0) > 1.0:
            reasons.append("contains specific dosages/measurements")
        
        if boosts.get('question_type', 1.0) > 1.0:
            reasons.append("matches question type pattern")
        
        if boosts.get('coherence', 1.0) > 1.0:
            reasons.append("well-structured content")
        
        if boosts.get('chunk_quality', 1.0) > 1.0:
            reasons.append("high-quality chunk")
        
        content_type = doc.metadata.get('content_type', '')
        if '[PROCEDURE]' in content_type:
            reasons.append("procedural content")
        elif '[TABLE]' in content_type:
            reasons.append("tabular data")
        
        return "; ".join(reasons) if reasons else "general relevance"
    
    # Main re-ranking logic
    enhanced_results = []
    
    for i, (doc, original_score) in enumerate(results):
        # Calculate various boost factors
        boosts = {
            'medical_keywords': get_medical_keyword_boost(doc.page_content, query_text),
            'medical_units': get_medical_units_boost(doc.page_content),
            'question_type': get_question_type_boost(doc.page_content, query_text),
            'coherence': get_coherence_score(doc.page_content, query_text),
            'diversity': get_diversity_score(results[:i], doc),
            'chunk_quality': get_chunk_quality_score(doc)
        }
        
        # Calculate enhanced score
        enhanced_score = original_score
        for boost_value in boosts.values():
            enhanced_score *= boost_value
        
        # Calculate confidence (0-1)
        confidence = min(enhanced_score / (original_score * 2.0), 1.0)
        
        # Generate reasoning
        reasoning = generate_reasoning(doc, boosts)
        
        enhanced_results.append((doc, enhanced_score, confidence, reasoning))
    
    # Sort by enhanced score (descending)
    enhanced_results.sort(key=lambda x: x[1], reverse=True)
    
    return enhanced_results


def query_rag(query_text: str, show_sources: bool = False):
    # Detect query type for adaptive processing
    query_type = detect_query_type(query_text)
    optimal_k = get_optimal_k(query_type)
    
    if show_sources:
        print(f"üîç Detected query type: {query_type.upper()}")
        print(f"üìä Using k={optimal_k} chunks for this query type")
    
    # Prepare the DB.
    embedding_function = get_embedding_function()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # Search the DB with adaptive k parameter
    initial_results = db.similarity_search_with_score(query_text, k=optimal_k + 2)  # Get extra for re-ranking

    if len(initial_results) == 0:
        print("Unable to find matching results.")
        return

    # Apply semantic re-ranking
    reranked_results = rerank_results(initial_results, query_text)
    
    # Take optimal number of results after re-ranking
    top_results = reranked_results[:optimal_k]
    
    # Extract documents and build context
    context_chunks = []
    for doc, enhanced_score, confidence, reasoning in top_results:
        context_chunks.append(doc.page_content)
    
    context_text = "\n\n---\n\n".join(context_chunks)
    
    # Use adaptive prompt template
    prompt = get_prompt_template(query_type, query_text, context_text)

    # For testing purposes without Ollama, create context-based response
    if query_type == "dosage":
        response_text = f"Th√¥ng tin li·ªÅu d√πng d·ª±a tr√™n t√†i li·ªáu y t·∫ø:\n\n{context_text[:800]}..."
    elif query_type == "symptom":
        response_text = f"C√°c tri·ªáu ch·ª©ng ƒë∆∞·ª£c ghi nh·∫≠n:\n\n{context_text[:800]}..."
    elif query_type == "procedure":
        response_text = f"Quy tr√¨nh th·ª±c hi·ªán:\n\n{context_text[:800]}..."
    elif query_type == "contraindication":
        response_text = f"‚ö†Ô∏è Th√¥ng tin an to√†n:\n\n{context_text[:800]}..."
    else:
        response_text = f"Based on the provided context, here's what I found:\n\n{context_text[:800]}..."
    
    # Apply type-specific formatting
    formatted_response = format_response_by_type(response_text, query_type)
    
    print(f"Response: {formatted_response}")
    
    if show_sources:
        print(f"\nüìä Detailed Source Analysis (Query Type: {query_type.upper()}):")
        for i, (doc, enhanced_score, confidence, reasoning) in enumerate(top_results):
            source_id = doc.metadata.get("id", "Unknown")
            content_type = doc.metadata.get("content_type", "[CONTENT]")
            original_relevance = doc.metadata.get("relevance_score", 0.5)
            
            print(f"\n  {i+1}. Source: {source_id}")
            print(f"     Type: {content_type}")
            print(f"     Confidence: {confidence:.2f}")
            print(f"     Original Relevance: {original_relevance:.2f}")
            print(f"     Reasoning: {reasoning}")
    
    return formatted_response


if __name__ == "__main__":
    main()
