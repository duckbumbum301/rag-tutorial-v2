#!/usr/bin/env python3
"""
Demo: How to use the Medical RAG Evaluation Framework
Demonstrates evaluating multiple models and comparing results
"""

from evaluation_framework import MedicalRAGEvaluator, TestQuestion
import pandas as pd

def demo_single_model_evaluation():
    """Demo: Evaluate a single model"""
    print("üî¨ DEMO 1: Single Model Evaluation")
    print("=" * 50)
    
    # Initialize evaluator for our baseline system
    evaluator = MedicalRAGEvaluator(models=["baseline"])
    
    # Load test dataset
    evaluator.load_test_dataset()
    print(f"üìã Loaded {len(evaluator.test_dataset)} test questions")
    
    # Evaluate the baseline model on first 3 questions (for demo speed)
    evaluator.test_dataset = evaluator.test_dataset[:3]
    print(f"üèÉ‚Äç‚ôÇÔ∏è Running evaluation on {len(evaluator.test_dataset)} questions for demo...")
    
    results = evaluator.evaluate_model("baseline")
    
    # Show results
    comparison_df = evaluator.compare_models()
    print("\\nüìä RESULTS:")
    print(comparison_df.round(3))
    
    return evaluator

def demo_metrics_explanation():
    """Demo: Explain what each metric means"""
    print("\\nüìö DEMO 2: Understanding Evaluation Metrics")
    print("=" * 60)
    
    print("""
üìà RETRIEVAL METRICS (How well we find relevant documents):
   ‚Ä¢ Recall@5: Did we retrieve documents containing the answer? (Higher = Better)
   ‚Ä¢ Precision@5: Were the top-5 documents relevant? (Higher = Better) 
   ‚Ä¢ MRR: How high was the first relevant document ranked? (Higher = Better)
   ‚Ä¢ NDCG: Overall ranking quality (Higher = Better)

üìù GENERATION METRICS (How well we generate answers):
   ‚Ä¢ ROUGE-L: Text similarity to reference answer (Higher = Better)
   ‚Ä¢ BERTScore: Semantic similarity (Higher = Better)
   ‚Ä¢ Exact Match: Did we get exact numbers right? (dosages, etc.)
   ‚Ä¢ Medical Accuracy: How many medical facts were correct? (Higher = Better)

‚ö° SPEED METRICS (How fast is the system):
   ‚Ä¢ Retrieval Latency: Time to find documents (Lower = Better)
   ‚Ä¢ Generation Latency: Time to generate answer (Lower = Better)
   ‚Ä¢ Total Latency: End-to-end response time (Lower = Better)

üõ°Ô∏è SAFETY METRICS (How safe are the answers):
   ‚Ä¢ Has Disclaimer: Does answer include medical warnings? (Higher = Better)
   ‚Ä¢ Cites Sources: Does answer reference source documents? (Higher = Better)
   ‚Ä¢ Hallucination Rate: % of answers with false info (Lower = Better)
   """)

def demo_custom_test_questions():
    """Demo: Adding custom test questions"""
    print("\\nüß™ DEMO 3: Adding Custom Test Questions")
    print("=" * 50)
    
    # Create a custom test question
    custom_question = TestQuestion(
        id="CUSTOM001",
        question="Li·ªÅu aspirin an to√†n cho tr·∫ª 5 tu·ªïi?",
        category="contraindication", 
        ground_truth={
            "safe": False,
            "reason": "Aspirin ch·ªëng ch·ªâ ƒë·ªãnh ·ªü tr·∫ª em d∆∞·ªõi 12 tu·ªïi",
            "answer": "KH√îNG an to√†n. Aspirin kh√¥ng ƒë∆∞·ª£c khuy·∫øn ngh·ªã cho tr·∫ª d∆∞·ªõi 12 tu·ªïi do nguy c∆° h·ªôi ch·ª©ng Reye."
        },
        expected_sources=["aspirin", "reye syndrome"],
        difficulty="medium"
    )
    
    print(f"üìù Custom Question: {custom_question.question}")
    print(f"   Category: {custom_question.category}")
    print(f"   Expected Answer: {custom_question.ground_truth['answer']}")
    
    # Evaluate single question
    evaluator = MedicalRAGEvaluator()
    try:
        result = evaluator.evaluate_single_question(custom_question, "baseline")
        print(f"\\n‚úÖ Evaluation Results:")
        print(f"   - Medical Accuracy: {result.medical_accuracy:.3f}")
        print(f"   - ROUGE-L Score: {result.rouge_l:.3f}")
        print(f"   - Response Time: {result.total_latency:.2f}s")
        print(f"   - Has Safety Warning: {result.has_disclaimer}")
    except Exception as e:
        print(f"‚ùå Evaluation failed: {e}")

def demo_performance_analysis():
    """Demo: Analyze performance by category"""
    print("\\nüìä DEMO 4: Performance Analysis by Category")  
    print("=" * 60)
    
    # Sample data showing different performance by question type
    sample_results = {
        'symptom': {'accuracy': 0.25, 'latency': 14.5, 'count': 5},
        'dosage': {'accuracy': 0.67, 'latency': 13.8, 'count': 3}, 
        'procedure': {'accuracy': 0.50, 'latency': 13.9, 'count': 2},
        'contraindication': {'accuracy': 0.75, 'latency': 14.2, 'count': 2}
    }
    
    print("üìà Performance by Medical Question Category:")
    print("-" * 60)
    for category, stats in sample_results.items():
        print(f"   {category.upper():<15} | Accuracy: {stats['accuracy']:.2f} | "
              f"Latency: {stats['latency']:.1f}s | Questions: {stats['count']}")
    
    print("\\nüí° INSIGHTS:")
    best_accuracy = max(sample_results.items(), key=lambda x: x[1]['accuracy'])
    worst_accuracy = min(sample_results.items(), key=lambda x: x[1]['accuracy'])
    fastest = min(sample_results.items(), key=lambda x: x[1]['latency'])
    
    print(f"   ‚Ä¢ Best Performance: {best_accuracy[0]} questions ({best_accuracy[1]['accuracy']:.2f} accuracy)")
    print(f"   ‚Ä¢ Needs Improvement: {worst_accuracy[0]} questions ({worst_accuracy[1]['accuracy']:.2f} accuracy)")
    print(f"   ‚Ä¢ Fastest Category: {fastest[0]} questions ({fastest[1]['latency']:.1f}s)")
    
    print("\\nüéØ RECOMMENDATIONS:")
    print("   ‚Ä¢ Dosage questions: Good accuracy - maintain current approach")
    print("   ‚Ä¢ Symptom questions: Low accuracy - improve medical terminology matching") 
    print("   ‚Ä¢ All categories: Latency >10s - optimize for clinical use")

def demo_benchmark_comparison():
    """Demo: Compare against benchmarks"""
    print("\\nüèÜ DEMO 5: Benchmark Comparison")
    print("=" * 50)
    
    # Simulated comparison with other medical RAG systems
    benchmark_data = {
        'System': ['Our Baseline', 'Commercial System A', 'Research System B', 'Target Goals'],
        'Medical Accuracy': [0.36, 0.72, 0.68, 0.80],
        'Avg Latency (s)': [14.4, 3.2, 8.1, 3.0],
        'Safety Score': [0.45, 0.89, 0.76, 0.90],
        'Coverage': ['12 questions', '100 questions', '50 questions', '1000+ questions']
    }
    
    df = pd.DataFrame(benchmark_data)
    print("üìä BENCHMARK COMPARISON:")
    print(df.to_string(index=False))
    
    print("\\nüìù ANALYSIS:")
    print("   ‚úÖ STRENGTHS:")
    print("      - System is functional across multiple medical domains")
    print("      - Retrieval pipeline working effectively")
    print("   ‚ö†Ô∏è  AREAS FOR IMPROVEMENT:")
    print("      - Medical accuracy significantly below commercial systems")
    print("      - Response latency 4-5x slower than target") 
    print("      - Safety mechanisms need enhancement")
    print("   üéØ NEXT STEPS:")
    print("      - Implement medical fact verification")
    print("      - Optimize inference pipeline for speed")
    print("      - Expand test coverage to 100+ questions")

def main():
    """Run all evaluation demos"""
    print("üöÄ MEDICAL RAG EVALUATION FRAMEWORK - DEMO")
    print("=" * 70)
    print("This demo shows how to use the evaluation framework to benchmark")
    print("medical RAG systems with scientific rigor.\\n")
    
    try:
        # Demo 1: Basic evaluation
        demo_single_model_evaluation()
        
        # Demo 2: Metrics explanation  
        demo_metrics_explanation()
        
        # Demo 3: Custom questions
        demo_custom_test_questions()
        
        # Demo 4: Performance analysis
        demo_performance_analysis()
        
        # Demo 5: Benchmark comparison
        demo_benchmark_comparison()
        
        print("\\nüéâ DEMO COMPLETE!")
        print("=" * 70)
        print("‚úÖ You now know how to:")
        print("   ‚Ä¢ Evaluate medical RAG systems scientifically")
        print("   ‚Ä¢ Compare multiple models objectively") 
        print("   ‚Ä¢ Analyze performance by medical question type")
        print("   ‚Ä¢ Benchmark against industry standards")
        print("   ‚Ä¢ Identify specific areas for improvement")
        print("\\nüî¨ Ready to benchmark your medical AI system!")
        
    except Exception as e:
        print(f"‚ùå Demo failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()